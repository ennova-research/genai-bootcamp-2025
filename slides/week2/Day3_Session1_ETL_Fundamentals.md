---
marp: true
title: ETL Fundamentals for AI Pipelines â€” v2
paginate: true
theme: default
header: Ennova Research â€¢ Data & AI XTDF Division
footer: GenAI Bootcamp â€” ETL Fundamentals for AI Pipelines
math: mathjax
---

# ğŸ§ª ETL Fundamentals for AI Pipelines
**Generative AI Bootcamp â€“ Week 2, Day 3, Session 1**
_November 26, 2025_

---

## ğŸ¯ Learning Objectives
- Understand ETL/ELT patterns across **batch** and **streaming**
- Map raw â†’ clean â†’ ready data for prompts and retrieval
- Choose the **right engine** for the **right scale/useâ€‘case**
- Identify quality checks and lineage practices
- See a concrete API â†’ table â†’ vector store pipeline

---

## ğŸ§­ ETL vs ELT
- **ETL:** Extract â†’ Transform â†’ Load (classic DW)
- **ELT:** Extract â†’ Load â†’ Transform (cloud-first, push-down to warehouses/lakes)
- AI context: prioritize **fast ingestion** + **iterative transforms** for prompting and eval

---

## ğŸ§± Pipeline Building Blocks
- **Extract:** APIs, files, event streams (Kafka, Kinesis, Pub/Sub)
- **Transform:** cleaning, normalization, token-aware trimming, embedding
- **Load:** SQL/NoSQL/vector stores; partitioning & metadata

---

## ğŸ§© Data for LLMs
- Structured: metadata, eval metrics, user ids
- Semi-structured: JSON prompts, configs
- Unstructured: docs, transcripts, embeddings
- Keep **provenance** and **versioning** for reproducibility

---

## ğŸ§¼ Common Transformations
- Type casting & normalization
- Deduplication (by key / text hash)
- Text cleaning (HTML, emojis, whitespace)
- PII scrubbing & anonymization
- Token-length gating for context windows

---

## ğŸ›¡ï¸ Data Quality & Validation
- Required columns present
- Non-null checks & allowed ranges
- Referential integrity across tables
- Assertions locally; **scalable checks** in orchestrated runs
- Tools: **Great Expectations, Deequ, Pandera, Pydantic**

---

## ğŸ§° Transform Engines â€” pick by scale & latency
Key heuristic: **push compute to where the data lives** (warehouse or lake).

### 1. Local / Small
For development or when <~1â€“5M rows.
- **pandas**: single machine, memory-bound
- **DuckDB**: SQL-in-process, great for Parquet/CSV joins
- **Polars**: fast, multi-threaded; lazy queries

---

## ğŸ§° Transform Engines â€” pick by scale & latency
### 2. Medium
Single host or small cluster.
- **Dask / Ray Data** (scale pandas-like APIs across cores/nodes)
- **Spark on a small cluster** for wide joins / shuffles

---

## ğŸ§° Transform Engines â€” pick by scale & latency
### 3. Large
Cluster, lake/warehouse.
- **Apache Spark / Databricks**
- **BigQuery / Snowflake / Redshift (ELT w/ SQL)**
- **Flink / Beam** for stateful & streaming with batch unification

---

## ğŸ›°ï¸ Streaming vs Batch
- **Batch**: scheduled, backfills, reprocessing large historical sets
- **Streaming**: low-latency updates, CDC, online features
- Buses: **Kafka, Kinesis, Pub/Sub**; Engines: **Flink, Beam, Spark Structured Streaming, Kafka Streams**

Examples:
- Nearâ€‘realâ€‘time moderation â†’ **Flink/Beam**
- Hourly RAG index refresh â†’ **Spark or warehouse SQL**
- Heavy backfills â†’ **Spark/Databricks** or **BigQuery/Snowflake** SQL

---

## ğŸ—ƒï¸ Storage Targets
- **OLAP / Warehouses**: BigQuery, Snowflake, Redshift
- **Data Lakes / Lakehouses**: S3, GCS + Delta, Apache Iceberg, Hudi
- **OLTP / Application DBs**: Postgres, MySQL, MongoDB
- **Search / Vector**: Elasticsearch/OpenSearch, **pgvector**, FAISS, Milvus, Pinecone, Weaviate

---

## ğŸ§­ Transform Authoring

- **SQL-first ELT** (dbt, SQLMesh, Dataform) â†’ warehouse-native, declarative, tested
- **Python-first** (pandas, Polars, Dask, Spark) â†’ code-based, complex or text-heavy logic
- **Unified batch/stream** (Beam, Flink) â†’ for low-latency or event-based ETL

---

## ğŸ§¬ Lineage & Metadata
- Track: source URL, fetch time, schema version, transformation hash
- Emit OpenLineage events; catalog with **DataHub**, **Marquez**, **Amundsen**
- Store columns like `_ingested_at`, `_source`, `_version`

---

## ğŸ›ï¸ Orchestration & Scheduling
- **Airflow**: mature DAGs, strong ecosystem
- **Prefect**: Pythonic flows, great observability
- **Dagster**: software-defined assets, type-checked IO
- Add **retries**, **backoff**, **idempotent** loads (UPSERT/MERGE)

---

## ğŸ“ When to Use What (quick guide)
- **Notebook exploration / small batch** â†’ pandas / Polars / DuckDB
- **Weekly/Monthly heavy batch** â†’ Spark on lakehouse; or ELT in BigQuery/Snowflake (dbt)
- **Hourly incremental** â†’ warehouse MERGE with partitions or Spark jobs
- **Streaming / <1 min latency** â†’ Flink/Beam/Kafka Streams
- **Text-heavy LLM prep** â†’ Spark + UDFs, or Polars for speed; validate with Great Expectations

---

## âš™ï¸ Demo Pipeline (Today) â€” updated
- **Extract:** REST API â†’ JSON (posts/comments) or Kafka topic
- **Transform (choose by size):**
  - Small: DuckDB/Polars â†’ clean, normalize, validate (Pandera/GE)
  - Medium: Dask/Ray â†’ parallel cleaning + joins
  - Large: Spark SQL/DataFrames â†’ tokenization & embeddings at scale
- **Load:**
  - Warehouse tables (BigQuery/Snowflake) for curated data
  - Vector index (pgvector/FAISS/Pinecone) for retrieval
- **Schedule:** Prefect or Airflow; tests via dbt or GE

---

## ğŸ“¦ Staging â†’ Warehouse
- Use staging tables for raw dumps
- Run deterministic transforms into curated schemas
- Idempotent loads (UPSERT/MERGE by primary key)

---

## âš ï¸ Pitfalls (now with scale in mind)
- Silent schema drift â†’ contracts/tests (dbt/GE), schema registry for streams
- Overusing `SELECT *` â†’ breakage on drift & higher costs
- No UTFâ€‘8/text normalization â†’ broken tokenization
- Underestimating **shuffle** costs in distributed engines
- Forgetting incremental strategies & partition pruning

---

## ğŸ’¡ Summary
- Choose the **engine by data size and latency** needs
- Validate early; preserve provenance and lineage
- Prefer push-down ELT in warehouses when possible
- Keep pipelines **modular, idempotent, observable**

---

## ğŸ Whatâ€™s Next
- Hands-on: Build the API/stream â†’ curated table â†’ vector store ETL with two variants:
  - Small data: DuckDB + dbt seeds + GE
  - Large data: Spark + dbt models + Prefect
