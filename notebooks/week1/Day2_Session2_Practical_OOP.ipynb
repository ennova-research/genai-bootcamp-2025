{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0317115c",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§© Day 2 â€” Practical OOP in AI Systems\n",
    "\n",
    "**Goal:** Build a modular, extensible *text-retrieval* miniâ€‘pipeline that runs fully offline and showcases real OOP techniques you can reuse in agent or RAG systems.\n",
    "\n",
    "**What you'll practice**\n",
    "- Abstractions with `abc.ABC` and dependency injection\n",
    "- **Strategy** for interchangeable components (preprocessors, embedders, retrievers)\n",
    "- **Adapter** to wrap thirdâ€‘party or legacy components behind your interface\n",
    "- **Observer** for lifecycle events + **Mixin** for logging\n",
    "- **Decorator** for caching & timing\n",
    "- A tiny but real **TFâ€‘IDF** embedder (implemented from scratch) + **cosine similarity**\n",
    "- Clean tests and exercises to extend the design\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece6526",
   "metadata": {},
   "source": [
    "\n",
    "> This lab mirrors the architecture ideas from the previous session (abstractions, composition over inheritance, and patterns) and turns them into a fully working, testable miniâ€‘project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19ad85",
   "metadata": {},
   "source": [
    "## 1) Tiny corpus for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e655e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "CORPUS = {\n",
    "    \"doc1\": \"Object-oriented programming (OOP) organizes software as collaborating objects with state and behavior. Composition favors reuse over inheritance.\",\n",
    "    \"doc2\": \"Retrieval-Augmented Generation (RAG) combines a retriever that finds relevant context with a generator that produces answers.\",\n",
    "    \"doc3\": \"TF-IDF represents a document by weighting terms by their frequency and their rarity across the corpus. Cosine similarity compares vectors by angle.\",\n",
    "    \"doc4\": \"Mixins are small classes that provide reusable behaviors via multiple inheritance, such as logging or timing.\",\n",
    "    \"doc5\": \"In AI engineering, modular design enables swapping tokenizers, embedders, vector stores, and rerankers without changing high-level code.\",\n",
    "    \"doc6\": \"Observer pattern broadcasts events to interested listeners, enabling decoupled logging, metrics, and UI updates.\",\n",
    "}\n",
    "len(CORPUS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03813f27",
   "metadata": {},
   "source": [
    "## 2) Core interfaces (ABCs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a00276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from __future__ import annotations\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Iterable, Tuple, Callable, Any\n",
    "import math\n",
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "# ---------- Observer ----------\n",
    "class EventBus:\n",
    "    def __init__(self):\n",
    "        self._subs: Dict[str, List[Callable[[Dict[str, Any]], None]]] = {}\n",
    "    def subscribe(self, event: str, fn: Callable[[Dict[str, Any]], None]):\n",
    "        self._subs.setdefault(event, []).append(fn)\n",
    "    def publish(self, event: str, payload: Dict[str, Any]):\n",
    "        for fn in self._subs.get(event, []):\n",
    "            fn(payload)\n",
    "\n",
    "# ---------- Mixins ----------\n",
    "class LogMixin:\n",
    "    def log(self, *args):  # lightweight\n",
    "        print(\"[LOG]\", *args)\n",
    "\n",
    "class TimeMixin:\n",
    "    def timeit(self, label: str, fn: Callable, *a, **k):\n",
    "        t0 = time.perf_counter()\n",
    "        out = fn(*a, **k)\n",
    "        dt = (time.perf_counter() - t0) * 1000\n",
    "        print(f\"[TIMER] {label}: {dt:.2f} ms\")\n",
    "        return out\n",
    "\n",
    "# ---------- ABCs ----------\n",
    "class Preprocessor(ABC):\n",
    "    @abstractmethod\n",
    "    def clean(self, text: str) -> str: ...\n",
    "\n",
    "class Embedder(ABC):\n",
    "    @abstractmethod\n",
    "    def fit(self, corpus: Iterable[str]) -> None: ...\n",
    "    @abstractmethod\n",
    "    def encode(self, text: str) -> List[float]: ...\n",
    "\n",
    "class VectorIndex(ABC):\n",
    "    @abstractmethod\n",
    "    def add(self, doc_id: str, vector: List[float]): ...\n",
    "    @abstractmethod\n",
    "    def search(self, query_vec: List[float], k: int = 3) -> List[Tuple[str, float]]: ...\n",
    "\n",
    "class Retriever(ABC):\n",
    "    @abstractmethod\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Tuple[str, float]]: ...\n",
    "\n",
    "class Reranker(ABC):\n",
    "    @abstractmethod\n",
    "    def rerank(self, query: str, items: List[Tuple[str, float]]) -> List[Tuple[str, float]]: ...\n",
    "\n",
    "class Answerer(ABC):\n",
    "    @abstractmethod\n",
    "    def answer(self, query: str, context_docs: List[str]) -> str: ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e182008",
   "metadata": {},
   "source": [
    "## 3) Concrete strategies and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e733ccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# ---- Utilities ----\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    return re.findall(r\"[A-Za-z0-9]+\", text.lower())\n",
    "\n",
    "def cosine(a: List[float], b: List[float]) -> float:\n",
    "    num = sum(x*y for x, y in zip(a, b))\n",
    "    da = math.sqrt(sum(x*x for x in a))\n",
    "    db = math.sqrt(sum(y*y for y in b))\n",
    "    return 0.0 if da == 0 or db == 0 else num / (da * db)\n",
    "\n",
    "# ---- Preprocessors ----\n",
    "class BasicPreprocessor(Preprocessor):\n",
    "    STOPWORDS = {\"the\",\"a\",\"an\",\"and\",\"or\",\"to\",\"of\",\"in\",\"is\",\"are\",\"that\",\"with\",\"by\"}\n",
    "    def clean(self, text: str) -> str:\n",
    "        toks = [t for t in tokenize(text) if t not in self.STOPWORDS]\n",
    "        return \" \".join(toks)\n",
    "\n",
    "class NoopPreprocessor(Preprocessor):\n",
    "    def clean(self, text: str) -> str:\n",
    "        return text\n",
    "\n",
    "# ---- TF-IDF Embedder (from scratch) ----\n",
    "class TfidfEmbedder(Embedder, LogMixin):\n",
    "    def __init__(self):\n",
    "        self.vocab: Dict[str,int] = {}\n",
    "        self.idf: List[float] = []\n",
    "    def fit(self, corpus: Iterable[str]) -> None:\n",
    "        docs = [tokenize(doc) for doc in corpus]\n",
    "        vocab = {}\n",
    "        for doc in docs:\n",
    "            for t in doc:\n",
    "                if t not in vocab:\n",
    "                    vocab[t] = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        N = len(docs)\n",
    "        df = [0]*len(vocab)\n",
    "        for doc in docs:\n",
    "            seen = set(doc)\n",
    "            for t in seen:\n",
    "                df[vocab[t]] += 1\n",
    "        import math\n",
    "        self.idf = [math.log((N+1)/(df_i+1))+1 for df_i in df]\n",
    "        self.log(f\"Fitted TF-IDF on {N} docs, |vocab|={len(vocab)}\")\n",
    "    def encode(self, text: str) -> List[float]:\n",
    "        toks = tokenize(text)\n",
    "        tf = Counter(toks)\n",
    "        vec = [0.0]*len(self.vocab)\n",
    "        for t, c in tf.items():\n",
    "            if t in self.vocab:\n",
    "                idx = self.vocab[t]\n",
    "                vec[idx] = (c / len(toks)) * self.idf[idx]\n",
    "        return vec\n",
    "\n",
    "# ---- Adapter example ----\n",
    "class LegacyCountVectorizer:\n",
    "    # Pretend this is a thirdâ€‘party component with a different API.\n",
    "    def fit_corpus(self, docs: List[str]):\n",
    "        self.vocab = {}\n",
    "        for d in docs:\n",
    "            for t in tokenize(d):\n",
    "                self.vocab.setdefault(t, len(self.vocab))\n",
    "    def vectorize(self, text: str) -> List[float]:\n",
    "        v = [0.0]*len(self.vocab)\n",
    "        for t in tokenize(text):\n",
    "            if t in self.vocab:\n",
    "                v[self.vocab[t]] += 1.0\n",
    "        return v\n",
    "\n",
    "class CountVectorAdapter(Embedder):\n",
    "    def __init__(self, legacy: LegacyCountVectorizer):\n",
    "        self.legacy = legacy\n",
    "    def fit(self, corpus: Iterable[str]) -> None:\n",
    "        self.legacy.fit_corpus(list(corpus))\n",
    "    def encode(self, text: str) -> List[float]:\n",
    "        return self.legacy.vectorize(text)\n",
    "\n",
    "# ---- Vector index ----\n",
    "class BruteForceIndex(VectorIndex):\n",
    "    def __init__(self):\n",
    "        self._vecs: Dict[str, List[float]] = {}\n",
    "    def add(self, doc_id: str, vector: List[float]):\n",
    "        self._vecs[doc_id] = vector\n",
    "    def search(self, query_vec: List[float], k: int = 3) -> List[Tuple[str, float]]:\n",
    "        scores = [(doc_id, cosine(query_vec, vec)) for doc_id, vec in self._vecs.items()]\n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "# ---- Retriever ----\n",
    "class SimpleRetriever(Retriever):\n",
    "    def __init__(self, pre: Preprocessor, emb: Embedder, index: VectorIndex, bus: EventBus | None = None):\n",
    "        self.pre, self.emb, self.index, self.bus = pre, emb, index, bus\n",
    "    @lru_cache(maxsize=256)\n",
    "    def _encode_cached(self, text: str) -> List[float]:\n",
    "        return self.emb.encode(text)\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Tuple[str, float]]:\n",
    "        clean = self.pre.clean(query)\n",
    "        if self.bus: self.bus.publish(\"query.start\", {\"query\": query, \"clean\": clean})\n",
    "        qv = self._encode_cached(clean)\n",
    "        hits = self.index.search(qv, k=k)\n",
    "        if self.bus: self.bus.publish(\"query.done\", {\"query\": query, \"hits\": hits})\n",
    "        return hits\n",
    "\n",
    "# ---- Reranker (optional; here just identity passthrough) ----\n",
    "class IdentityReranker(Reranker):\n",
    "    def rerank(self, query: str, items: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
    "        return items\n",
    "\n",
    "# ---- Answerer ----\n",
    "class TemplateAnswerer(Answerer):\n",
    "    def __init__(self, corpus: Dict[str,str]):\n",
    "        self.corpus = corpus\n",
    "    def answer(self, query: str, context_docs: List[str]) -> str:\n",
    "        bullets = \"\\n\".join(f\"- {doc[:160]}...\" for doc in context_docs)\n",
    "        return f\"Answering: '{query}'\\n\\nTop context:\\n{bullets}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e31f256",
   "metadata": {},
   "source": [
    "## 4) Ingest & build the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c5ccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] Fitted TF-IDF on 6 docs, |vocab|=92\n",
      "Index ready. Documents: ['doc1', 'doc2', 'doc3', 'doc4', 'doc5', 'doc6']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BUS = EventBus()\n",
    "BUS.subscribe(\"query.start\", lambda e: print(f\"[EVENT] query.start clean='{e['clean']}'\"))\n",
    "BUS.subscribe(\"query.done\",  lambda e: print(f\"[EVENT] query.done hits={e['hits']}\"))\n",
    "\n",
    "pre = BasicPreprocessor()\n",
    "emb = TfidfEmbedder()\n",
    "emb.fit(CORPUS.values())\n",
    "\n",
    "index = BruteForceIndex()\n",
    "for doc_id, text in CORPUS.items():\n",
    "    index.add(doc_id, emb.encode(pre.clean(text)))\n",
    "\n",
    "retriever = SimpleRetriever(pre, emb, index, bus=BUS)\n",
    "reranker  = IdentityReranker()\n",
    "answerer  = TemplateAnswerer(CORPUS)\n",
    "\n",
    "print(\"Index ready. Documents:\", list(CORPUS.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523441f9",
   "metadata": {},
   "source": [
    "## 5) Endâ€‘toâ€‘end pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c294356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVENT] query.start clean='how does tf idf work why cosine similarity'\n",
      "[EVENT] query.done hits=[('doc3', 0.4588314677411235), ('doc1', 0.0), ('doc2', 0.0)]\n",
      "Answering: 'How does TF-IDF work and why cosine similarity?'\n",
      "\n",
      "Top context:\n",
      "- TF-IDF represents a document by weighting terms by their frequency and their rarity across the corpus. Cosine similarity compares vectors by angle....\n",
      "- Object-oriented programming (OOP) organizes software as collaborating objects with state and behavior. Composition favors reuse over inheritance....\n",
      "- Retrieval-Augmented Generation (RAG) combines a retriever that finds relevant context with a generator that produces answers....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_pipeline(query: str, k: int = 3):\n",
    "    hits = retriever.retrieve(query, k=k)\n",
    "    doc_ids = [doc_id for doc_id, _ in reranker.rerank(query, hits)]\n",
    "    contexts = [CORPUS[i] for i in doc_ids]\n",
    "    return answerer.answer(query, contexts)\n",
    "\n",
    "print(run_pipeline(\"How does TF-IDF work and why cosine similarity?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14f03c",
   "metadata": {},
   "source": [
    "## 6) Simple factory for embedders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56605605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('doc4', 0.4803844614152615), ('doc6', 0.16666666666666669), ('doc1', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_embedder(kind: str) -> Embedder:\n",
    "    kind = kind.lower()\n",
    "    if kind == \"tfidf\":\n",
    "        return TfidfEmbedder()\n",
    "    elif kind == \"count\":\n",
    "        return CountVectorAdapter(LegacyCountVectorizer())\n",
    "    raise ValueError(f\"Unknown embedder kind: {kind}\")\n",
    "\n",
    "# Demo swap:\n",
    "emb2 = make_embedder(\"count\")\n",
    "emb2.fit(CORPUS.values())\n",
    "index2 = BruteForceIndex()\n",
    "for i, t in CORPUS.items():\n",
    "    index2.add(i, emb2.encode(pre.clean(t)))\n",
    "retriever2 = SimpleRetriever(pre, emb2, index2)\n",
    "print([x for x in retriever2.retrieve(\"mixins for logging and timing\", k=3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119cc886",
   "metadata": {},
   "source": [
    "## 7) Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53e94da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVENT] query.start clean='what oop'\n",
      "[EVENT] query.done hits=[('doc1', 0.2552811336748491), ('doc2', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import unittest\n",
    "\n",
    "class TestMiniPipeline(unittest.TestCase):\n",
    "    def test_tokenize(self):\n",
    "        self.assertEqual(tokenize(\"A,B!\"), [\"a\",\"b\"])\n",
    "    def test_cosine_unit(self):\n",
    "        self.assertAlmostEqual(cosine([1,0],[1,0]), 1.0, places=6)\n",
    "        self.assertAlmostEqual(cosine([1,0],[0,1]), 0.0, places=6)\n",
    "    def test_retrieval_nonempty(self):\n",
    "        hits = retriever.retrieve(\"What is OOP?\", k=2)\n",
    "        self.assertTrue(len(hits) == 2 and all(isinstance(h[1], float) for h in hits))\n",
    "\n",
    "unittest.TextTestRunner().run(unittest.defaultTestLoader.loadTestsFromTestCase(TestMiniPipeline));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced819c5",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Exercises (âœï¸ you edit these cells)\n",
    "\n",
    "1. **Add a new Preprocessor strategy** that stems words (you can implement a tiny stemmer that drops common suffixes like `ing`, `ed`, `s`).\n",
    "2. **Implement a BM25Retriever** (hint: compute IDF as above; use Okapi BM25 formula with `k1=1.2`, `b=0.75`).\n",
    "3. **Add a `CacheMixin`** that decorates `VectorIndex.search` with an LRU cache keyed by `(hash(query_vec), k)`.\n",
    "4. **Observer drill:** emit `ingest.start`/`ingest.done` events during indexing and print basic metrics.\n",
    "5. **Reranker:** implement a queryâ€‘document *bigram overlap* reranker and compare with identity.\n",
    "6. **Adapter:** pretend you received embeddings from a remote vector service shaped as dicts `{\"embedding\": [...], \"dim\": N}`. Write `RemoteVectorAdapter` that conforms to `Embedder`.\n",
    "7. **Testing:** write tests for your new components in section 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.1) New Preprocessor (Stemmer) â€” TODO\n",
    "class StemPreprocessor(Preprocessor):\n",
    "    def clean(self, text: str) -> str:\n",
    "        # TODO: implement very tiny rule-based stemmer\n",
    "        return text  # replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed6cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.2) BM25 Retriever â€” TODO\n",
    "class BM25Retriever(Retriever):\n",
    "    def __init__(self, pre: Preprocessor, index_docs: Dict[str, str]):\n",
    "        # TODO: precompute bm25 stats\n",
    "        self.pre = pre\n",
    "        self.docs = index_docs\n",
    "    def retrieve(self, query: str, k: int = 3) -> List[Tuple[str, float]]:\n",
    "        # TODO: compute BM25 scores\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c357a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.3) CacheMixin for search â€” TODO\n",
    "class CacheMixin:\n",
    "    # TODO: wrap an existing index.search with lru_cache\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1099fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.4) Observer instrumentation for ingest â€” TODO\n",
    "def build_index_with_events(pre: Preprocessor, emb: Embedder, corpus: Dict[str,str], bus: EventBus) -> VectorIndex:\n",
    "    # TODO: publish ingest.start / ingest.done with timings and counts\n",
    "    index = BruteForceIndex()\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fccfe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.5) Bigram overlap Reranker â€” TODO\n",
    "class BigramOverlapReranker(Reranker):\n",
    "    def rerank(self, query: str, items: List[Tuple[str, float]]) -> List[Tuple[str, float]]:\n",
    "        return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe084e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.6) RemoteVectorAdapter â€” TODO\n",
    "class RemoteVectorAdapter(Embedder):\n",
    "    def fit(self, corpus: Iterable[str]) -> None:\n",
    "        pass  # pretend remote doesn't need fit\n",
    "    def encode(self, text: str) -> List[float]:\n",
    "        # TODO: call a fake remote and adapt to list[float]\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e821b3d",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Bonus â€” Swappable UIs\n",
    "\n",
    "Because the pipeline is cleanly separated, you can build multiple clients:\n",
    "- a CLI (input loop)\n",
    "- a simple Streamlit/Gradio app\n",
    "- a batch evaluator that runs a small set of queries and prints topâ€‘k docs\n",
    "\n",
    "Try it as a challenge after class. ðŸ‘‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cli_demo():\n",
    "    print(\"Type a query (or 'quit'):\")\n",
    "    while True:\n",
    "        q = input(\"> \").strip()\n",
    "        if q.lower() in {\"q\",\"quit\",\"exit\"}:\n",
    "            break\n",
    "        print(run_pipeline(q, k=3), \"\\n\")\n",
    "\n",
    "# Uncomment to try in an interactive session:\n",
    "# cli_demo()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
