{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”Œ API Exploration Lab\n",
        "Day 2 â€“ Session 2\n",
        "\n",
        "In this notebook we will authenticate, send API requests, inspect responses, and implement simple robustness patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup & Credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "from typing import Dict, Any\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "if not OPENAI_API_KEY:\n",
        "    print('âš ï¸ Set OPENAI_API_KEY in your environment to run live calls.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Helper: Safe API Generate Function with Retries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import time, random\n",
        "\n",
        "def generate(prompt: str, *, model: str='gpt-4o-mini', temperature: float=0.7, max_tokens: int=200, retries: int=3, backoff: float=0.8) -> str:\n",
        "    \"\"\"Call the chat completion API with basic retries and timing.\n",
        "    Returns the model's answer as plain text.\n",
        "    \"\"\"\n",
        "    ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Compare Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = 'Write three product taglines for a note-taking app.'\n",
        "\n",
        "try:\n",
        "    out1 = generate(prompt, temperature=0.2)\n",
        "    out2 = generate(prompt, temperature=0.9)\n",
        "    print('â€” Low temperature (0.2):\\n', out1)\n",
        "    print('\\nâ€” High temperature (0.9):\\n', out2)\n",
        "\n",
        "except Exception as e:\n",
        "    raise # one would take the opportunity to do something more meaningful..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Add usage and latency metadata\n",
        "\n",
        "Modify the function above so that it returns usage and latency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate(prompt: str, *, model: str='gpt-4o-mini', temperature: float=0.7, max_tokens: int=200, retries: int=3, backoff: float=0.8) -> Dict[str, Any]:\n",
        "    \"\"\"Call the chat completion API with basic retries and timing.\n",
        "    Returns dict with text, usage (if available), and latency_ms.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "out = generate(prompt)\n",
        "print('Usage:', json.dumps(out['usage']))\n",
        "print('Latency (ms):', out['latency_ms'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) More exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Modify `generate` to accept a list of messages (system, user) and a stop sequence.\n",
        "2. Add structured logging (JSON lines).\n",
        "3. Try two prompts and compare responses for tone and length."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
